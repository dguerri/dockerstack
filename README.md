## A picture is worth a thousand words

![A picture is worth a thousand words](doc/demo.png)

## Prerequisites

 * [Docker](https://www.docker.com) 1.6.0 or later

 * [docker-py](https://github.com/docker/docker-py) version 1.2.3 on the ansible server running the playbook
 * [MySQL-python](https://pypi.python.org/pypi/MySQL-python) on the ansible server running the playbook
 * OpenStack python clients for Keystone, Glance, Neutron, Swift, Ironic and Nova on the ansible server running the playbook

        pip install python-{keystone,neutron,ironic,nova,glance,swift}client docker-py==1.2.3 MySQL-python
    

If you are using an Ubuntu box, above requirements require in turn:

    apt-get install libmysqlclient-dev libxml2-dev libxslt1-dev


If you are going to build the containers behind a proxy (_not recommended_), you will have to tweak both the Docker default configuration file and the os-base-image Dockerfile. [Here](http://nknu.net/running-docker-behind-a-proxy-on-ubuntu-14-04/) is a good guide about that.

## Clean

    # Clean version 1.0 with 5 parallel processes
    make clean -j5 BUILD_VERSION=1.0

## Test

Testing requires [shellcheck](http://www.shellcheck.net/about.html) 1.3.8 or later.

    # Test version 1.0 with 5 parallel processes
    make test -j5 BUILD_VERSION=1.0

### Example output

    (davide:marley)-[0]-(~/D/openstack-docker) # make test
    â˜ï¸  os-mysql:latest - Not implemented
    â˜ï¸  os-httpboot:latest - Not implemented
    â˜ï¸  os-tftpboot:latest - Not implemented
    â˜ï¸  os-rabbitmq:latest - Not implemented
    â˜ï¸  os-memcached:latest - Not implemented
    âœ…  os-keystone:latest - Passed
    âœ…  os-glance-registry:latest - Passed
    âœ…  os-glance-api:latest - Passed
    âœ…  os-neutron-server:latest - Passed
    âœ…  os-nova-conductor:latest - Passed
    âœ…  os-nova-api:latest - Passed
    âœ…  os-nova-scheduler:latest - Passed
    âœ…  os-nova-compute:latest - Passed
    âœ…  os-neutron-dhcp-agent:latest - Passed
    âœ…  os-ironic-conductor:latest - Passed
    âœ…  os-ironic-api:latest - Passed
    âœ…  os-swift-proxy:latest - Passed
    âœ…  os-swift-account:latest - Passed
    âœ…  os-swift-object:latest - Passed
    âœ…  os-swift-container:latest - Passed
    â˜ï¸  os-base-image:latest - Not implemented

## Build

    # Build "latest"
    make all

    # Build version 1.0
    make all BUILD_VERSION=1.0

### Example output

    (davide:marley)-[0]-(~/D/openstack-docker) # make all
    ğŸ”¨  os-base-image:latest - Done
    ğŸ”¨  os-mysql:latest - Done
    ğŸ”¨  os-httpboot:latest - Done
    ğŸ”¨  os-tftpboot:latest - Done
    ğŸ”¨  os-rabbitmq:latest - Done
    ğŸ”¨  os-memcached:latest - Done
    ğŸ”¨  os-keystone:latest - Done
    ğŸ”¨  os-glance-registry:latest - Done
    ğŸ”¨  os-glance-api:latest - Done
    ğŸ”¨  os-neutron-server:latest - Done
    ğŸ”¨  os-nova-conductor:latest - Done
    ğŸ”¨  os-nova-api:latest - Done
    ğŸ”¨  os-nova-scheduler:latest - Done
    ğŸ”¨  os-nova-compute:latest - Done
    ğŸ”¨  os-neutron-dhcp-agent:latest - Done
    ğŸ”¨  os-ironic-conductor:latest - Done
    ğŸ”¨  os-ironic-api:latest - Done
    ğŸ”¨  os-swift-proxy:latest - Done
    ğŸ”¨  os-swift-account:latest - Done
    ğŸ”¨  os-swift-object:latest - Done
    ğŸ”¨  os-swift-container:latest - Done

## Run the demo

### Please note
Please keep in mind that the "interesting" things here are the Dockerfiles, not the demo. The demo shows just _one_ possible way to use container images that are built with the previous command and it is very dependent on the hardware I used to develop it.

Also, in a production environment you may want to distribute your containers on multiple servers and use an external DNS server keeping track of the needed aliases ("service names" -> docker server). You may want to use a DNS service with an API service orchestrated with Ansible, for instance.

The included Ansible playbook also creates a number of data containers to demonstrate how data can be persisted across upgrades while preserving portability.

### Demo
The included demo consists of an Ansible playbook and 2 shell scripts. It is designed for Parallels Desktop and requires an unprovisioned virtual machine with one NIC with MAC address `00:1C:42:89:64:34`.

The demo also uses [autodns](https://github.com/rehabstudio/docker-autodns) from rehabstudio. This is not a strict requirement for the proposed infrastructure so you can use your preferred DNS, as long as it can be configured dynamically during the creation of containers. 

Using Ansible to configure an external DNS or even using Avahi daemon are possible alternatives.

For the sake of this demo, as described [here](https://github.com/rehabstudio/docker-autodns#prerequisites), the docker daemon should be started with the following parameters:

        DOCKER_OPTS="--bip=172.17.42.1/16 --dns=172.17.42.1 --dns=<your resolver1> [--dns=<your resolver2> [...]]"

 Add `nameserver 127.0.0.1` on top of the resolv.conf file running the docker server.

__Run the demo:__

1. Run `ansible-playbook`:

        ~# cd ansible
        ~# time ansible-playbook -i inventory/docker_server site.yml

 After a successful play, you should have the following list of containers (output of `docker ps -a`, edited):

        IMAGE                   PORTS                      NAMES
        os-glance-api           0.0.0.0:9292->9292/tcp     glance-api.os-in-a-box
        os-glance-registry      9191/tcp                   glance-registry.os-in-a-box
        os-swift-proxy          0.0.0.0:8080->8080/tcp     swift-proxy.os-in-a-box
        os-swift-object         6000/tcp                   swift-object.os-in-a-box
        os-swift-container      6001/tcp                   swift-container.os-in-a-box
        os-swift-account        6002/tcp                   swift-account.os-in-a-box
        os-base-image                                      swift-devs-data
        os-base-image                                      swift-rings-data
        os-nova-compute                                    nova-compute.os-in-a-box
        os-nova-scheduler                                  nova-scheduler.os-in-a-box
        os-nova-api             0.0.0.0:8774->8774/tcp     nova-api.os-in-a-box
        os-nova-conductor                                  nova-conductor.os-in-a-box
        os-neutron-dhcp-agent                              neutron-dhcp-agent.os-in-a-box
        os-neutron-server       0.0.0.0:9696->9696/tcp     neutron-server.os-in-a-box
        os-ironic-api           0.0.0.0:6385->6385/tcp     ironic-api.os-in-a-box
        os-ironic-conductor                                ironic-conductor.os-in-a-box
        os-httpboot             0.0.0.0:8090->80/tcp       ipxe-httpd.os-in-a-box
        os-tftpboot             0.0.0.0:69->69/udp         pxe-tftp.os-in-a-box
        os-base-image                                      pxe-boot-data
        os-rabbitmq             5672/tcp                   rabbitmq.os-in-a-box
        os-keystone             0.0.0.0:5000->5000/tcp,    keystone.os-in-a-box
                                0.0.0.0:35357->35357/tcp
        os-mysql                3306/tcp                   mysql.os-in-a-box
        os-base-image                                      mysql-data
        os-memcached            11211/tcp                  memcached.os-in-a-box
        rehabstudio/autodns     0.0.0.0:53->53/udp         autodns.os-in-a-box

2. run `scripts/connect_external_net.sh` to attach `eth1` (an external physical interface) to the provisioning network.
This also creates a virtual switch and a couple of veth interfaces. Il also "pushes" one of the 2 veth interface in the `neutron-dhcp-agent` container.

 The following picture shows the final (virtual) networking configuration after running `scripts/connect_external_net.sh`:

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”
        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚ â”‚â”€â”‚BM nodeâ”‚
        â”‚ â”‚                  docker0                   â”‚   â”‚   â”‚pâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚hâ”‚          
        â”‚           â”‚                       â”‚              â”‚   â”‚yâ”‚          
        â”‚           â”‚                       â”‚              â”‚   â”‚sâ”‚          
        â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚   â”‚iâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”
        â”‚â”‚ Neutron DHCP Agent â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”    â”‚   â”‚câ”‚â”€â”‚BM nodeâ”‚
        â”‚â”‚     Container      â”‚    â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”  â”‚   â”‚aâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚    â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”â”‚   â”‚lâ”‚          
        â”‚â”‚â”‚      br-exâ”Œâ”€â”€â”€â”€â” â”‚â”‚    â”‚ â”‚ â”‚ â”‚Other containersâ”‚â”‚   â”‚-â”‚          
        â”‚â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ext1â”œâ”€â”˜â”‚    â””â”€â”¤ â”‚ â”‚                â”‚â”‚   â”‚nâ”‚          
        â”‚â”‚            â””â”€â”€â”€â”€â”˜  â”‚      â””â”€â”¤ â”‚                â”‚â”‚   â”‚eâ”‚          
        â”‚â”‚               â”‚    â”‚        â””â”€â”¤                â”‚â”‚   â”‚tâ”‚          
        â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚   â”‚wâ”‚          
        â”‚                â”‚                                 â”‚   â”‚oâ”‚          
        â”‚             â”Œâ”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”´â”€â” â”‚râ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”
        â”‚             â”‚ext0â”‚â”€â”€â”€â”€â”€â”€â”‚ provisioning  â”‚â”€â”€â”€â”€â”€â”‚eth1â”‚â”€â”‚kâ”‚â”€â”‚BM nodeâ”‚
        â”‚     â”Œâ”€â”€â”€â”€â”  â””â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”¬â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”˜
        â””â”€â”€â”€â”€â”€â”¤eth0â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”˜          
              â””â”€â”€â”€â”€â”˜                                                        

3. run `scripts/setup_openstack.sh` to create the initial demo setup for BM provisioning.

